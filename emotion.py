# -*- coding: utf-8 -*-
"""emotion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zFW6p6GeJIUId7t_7bOwT429ooBxWRp0
"""


from fastai.vision.all import *
from fastai.metrics import error_rate, accuracy





from fastai.data.core import DataLoaders

import os
import zipfile

# local_zip = '/content/fer2013.zip'
# zip_ref.extractall('emotions_recogination')
# zip_ref.close()

# df=pd.read_csv('/content/emotions_recogination/fer2013.csv')
# df['Usage'].unique()
# df

# path=Path('/content/emotions_recogination')



class EmotionImage(fastuple):
    def show(self, ctx=None, **kwargs):
        img, emotion = self
        if not isinstance(img, Tensor):
          image=tensor(img).float()/255.0
          image=image.unsqueeze(0)
          image=image.expand(3,48,48)
          
          img_tensor = image
        else:
          img_tensor = img
        return show_image(img_tensor,title=emotion, ctx=ctx, **kwargs)

width,height=48,48
image_size=(48,48)
class EmotionDataset2(Transform):
  def __init__(self,df):
    super(EmotionDataset2,self).__init__()
    self.df=df
  

  def __len__(self):
    return len(self.df)

  def encodes(self,idx):
    # print(idx)
    idx=idx.name
  
    torch.Tensor.ndim = property(lambda x: len(x.shape))
    # print(idx.name)
    # emotion=idx['emotion']
    emotion=df['emotion'].iloc[idx]

    torch.Tensor.ndim = property(lambda x: len(x.shape))
    one_hot_targets = torch.tensor(np.eye(7)[emotion]).type(torch.FloatTensor)
  
    arr=df['pixels'].iloc[idx]
    # print(arr)
    # arr=list(map(int,pixels.split(' ')))

    array=np.array(arr).reshape(width, height)
    img= Image.fromarray((array).astype('uint8'))
    # image=to_image(img)
    image=PILImage(img)
    
    return EmotionImage(image ,one_hot_targets)



@typedispatch
def show_batch(x:EmotionImage, y, samples, ctxs=None, max_n=6, nrows=None, ncols=2, figsize=None, **kwargs):
    if figsize is None: figsize = (ncols*6, max_n//ncols * 3)
    if ctxs is None: ctxs = get_grid(min(x[0].shape[0], max_n), nrows=None, ncols=ncols, figsize=figsize)
    for i,ctx in enumerate(ctxs): EmotionImage((x[0][i], str( torch.max(x[1][i]).item() ) ) ).show(ctx=ctx)

# for i in range(len(df)):
#   pixels=df['pixels'].iloc[i]
#   arr=list(map(int,pixels.split(' ')))
#   df['pixels'].iloc[i]=np.array(arr)

# df.head()

# splits = RandomSplitter()(df)
# # splits
# tfm = EmotionDataset2(df)
# splits

# tls = TfmdLists(df, tfm,splits=splits)

# show_at(tls.valid, 0)

# dls = tls.dataloaders(after_item=[ToTensor], 
#                       batch_tfms=[Normalize.from_stats(*imagenet_stats)])
# dls.cuda()

# dls = tls.dataloaders(after_item=[ToTensor], 
#                       after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)])

# dls.cuda()

# b = dls.one_batch()
# b

# dls._types,type(b)

# dls.show_batch()



# dls.c=7

# learn=cnn_learner(dls,arch=models.resnet18, loss_func=nn.BCEWithLogitsLoss(), opt_func=Adam,metrics=accuracy_multi, cbs=CudaCallback)

# x,y = to_cpu(dls.train.one_batch())
# print(x.shape)
# activs = learn.model(x)
# activs

# dls.show_batch()

# learn.lr_find()

# learn.freeze()

# learn.fit_one_cycle(15,1e-02)



# learn.unfreeze()

# learn.lr_find()

# learn.fit_one_cycle(5,1e-06)

# # intrep=Interpretation.from_learner(learn)

# # intrep.plot_top_losses(9, figsize=(15,10))

# # learn.show_results()

# learn.freeze()

# learn.export()

# # i=PILImage.create("/content/download (1).png",mode='L')

# # m=PILImage(i)
# # print(type(m))

# # # k=EmotionImage(i)

# # pred=learn.predict(k)

# # p=torch.argmax(pred[0])

# # id=p.item()

# # objects = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']

# # objects[2]

